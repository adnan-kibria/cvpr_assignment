{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1",
      "mount_file_id": "1yD7SLqmR0U6wArF2dphnhM0gHqiHIJx6",
      "authorship_tag": "ABX9TyO/jXSR5tBcXlKCso58s/Ca",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adnan-kibria/cvpr_assignment/blob/main/MID/assignment_2_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# imports necessary libraries.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "ubV5gW3faZ6e"
      },
      "execution_count": 385,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount google drive to get access of dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fR05rw6t5zl",
        "outputId": "385bde56-2efb-4651-912e-74a3dddacff0"
      },
      "execution_count": 386,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and read dataset with the help of pandas\n",
        "path = '/content/drive/MyDrive/Dataset/random_dataset/random_dataset.csv'\n",
        "df = pd.read_csv(path)\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDI34Cv7akyi",
        "outputId": "308cd008-a25f-499f-aac5-8cdc53acec10"
      },
      "execution_count": 387,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   feature_1  feature_2  feature_3  feature_4  class\n",
            "0   1.611326   5.042269   3.633166  -2.673706      1\n",
            "1   5.546934   1.643341  -4.627222  -2.796811      4\n",
            "2   3.252296   5.100132  -1.982558  -3.270648      3\n",
            "3   2.955725   3.848868   4.737395  -2.433979      1\n",
            "4  -4.268004   2.344520  -1.539793   1.221274      2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the input features\n",
        "X = df[['feature_1', 'feature_2', 'feature_3', 'feature_4']].values\n",
        "print(X.shape)\n",
        "# YRaw originally contains class labels in the dataset.\n",
        "YRaw = df[['class']].values.flatten()\n",
        "# Convert labels from 1–5 to 0–4 range to match Python's zero-based indexing\n",
        "# Formula: y_new = y_old - 1\n",
        "YRaw = YRaw - 1\n",
        "# Indexing this with YRaw produces one-hot encoded labels.\n",
        "Y = np.eye(5)[YRaw]"
      ],
      "metadata": {
        "id": "VlcgT-voanhj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "757e457e-e7bd-43d9-ecf2-c125ecef948b"
      },
      "execution_count": 388,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(500, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementation of 3 Hidden Layer Neural Network\n",
        "# Normalize features to range [0,1].\n",
        "# Formula: X_norm = (X - min) / (max - min)\n",
        "class NeuralNetwork(object):\n",
        "    def __init__(self):\n",
        "        # Number of neurons in each layer\n",
        "        inputLayerNeurons = 4\n",
        "        hiddenLayer1Neurons = 32\n",
        "        hiddenLayer2Neurons = 16\n",
        "        hiddenLayer3Neurons = 8\n",
        "        outputLayerNeurons = 5 # Multi-class classification (5 classes)\n",
        "\n",
        "        # Learning rate for gradient descent\n",
        "        self.learningRate = 0.001\n",
        "        self.W_HI1 = np.random.randn(inputLayerNeurons, hiddenLayer1Neurons) * 0.01\n",
        "        self.BI1 = np.zeros((1, hiddenLayer1Neurons))\n",
        "        self.W_HI2 = np.random.randn(hiddenLayer1Neurons, hiddenLayer2Neurons) * 0.01\n",
        "        self.BI2 = np.zeros((1, hiddenLayer2Neurons))\n",
        "        self.W_HI3 = np.random.randn(hiddenLayer2Neurons, hiddenLayer3Neurons) * 0.01\n",
        "        self.BI3 = np.zeros((1, hiddenLayer3Neurons))\n",
        "        self.W_OH = np.random.randn(hiddenLayer3Neurons, outputLayerNeurons) * 0.01\n",
        "        self.BI4 = np.zeros((1, outputLayerNeurons))\n",
        "\n",
        "    #ReLU Activation Function\n",
        "    # Formula: ReLU(x) = max(0, x)\n",
        "    # Derivative: ReLU'(x) = 1 if x > 0 else 0\n",
        "    def ReLU(sef, x, der = False):\n",
        "        if der == True:\n",
        "            return (x > 0).astype(float)\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    # Softmax Activation Function\n",
        "    # Formula: softmax(z_i) = exp(z_i) / Σ exp(z_j)\n",
        "    # Used in output layer for multi-class classification\n",
        "    def Softmax(self, x):\n",
        "        exp = np.exp(x - np.max(x, axis = 1, keepdims = True))\n",
        "        return exp / np.sum(exp, axis = 1, keepdims = True)\n",
        "\n",
        "    def forwardPropagation(self, X):\n",
        "        \"\"\"\n",
        "        Forward propagation through 3 hidden layers + output layer.\n",
        "\n",
        "        Layer computations:\n",
        "        hiddenInput1 = ReLU(X * W_HI1 + BI1)\n",
        "        hiddenInput2 = ReLU(hiddenLayer1Output * W_HI2 + BI2)\n",
        "        hiddenInput3 = ReLU(hiddenLayer2Output * W_HI3 + BI3)\n",
        "        prediction = Softmax(hiddenLayer3Output * W_OH + BI4)\n",
        "        \"\"\"\n",
        "        hiddenInput1 = np.dot(X, self.W_HI1) + self.BI1\n",
        "        self.hiddenOutput1 = self.ReLU(hiddenInput1)\n",
        "        hiddenInput2 = np.dot(self.hiddenOutput1, self.W_HI2) + self.BI2\n",
        "        self.hiddenOutput2 = self.ReLU(hiddenInput2)\n",
        "        hiddenInput3 = np.dot(self.hiddenOutput2, self.W_HI3) + self.BI3\n",
        "        self.hiddenOutput3 = self.ReLU(hiddenInput3)\n",
        "        outputInput = np.dot(self.hiddenOutput3, self.W_OH) + self.BI4\n",
        "        prediction = self.Softmax(outputInput)\n",
        "\n",
        "        return prediction\n",
        "\n",
        "    def backPropagation(self, X, Y, prediction):\n",
        "        \"\"\"\n",
        "        Cross-entropy loss used here:\n",
        "        L = - Σ_i  y_i * log(ŷ_i)\n",
        "\n",
        "        For Softmax + Cross-Entropy:\n",
        "        dL/dz = ŷ - y    (simplified gradient)\n",
        "        \"\"\"\n",
        "\n",
        "        # Output layer gradient (Softmax + Cross Entropy derivative)\n",
        "        outputDelta = (prediction - Y)\n",
        "\n",
        "        # Gradients for output weights and biases\n",
        "        gW_OH = np.dot(self.hiddenOutput3.T, outputDelta)\n",
        "        gBI4 = np.sum(outputDelta, axis = 0, keepdims = True)\n",
        "\n",
        "        # Backprop into 3rd hidden layer\n",
        "        hiddenOutput3Loss = np.dot(outputDelta, self.W_OH.T)\n",
        "        hiddenOutput3Delta = hiddenOutput3Loss * self.ReLU(self.hiddenOutput3, der = True)\n",
        "\n",
        "        # Gradients for output gW_HI3 and gBI3\n",
        "        gW_HI3 = np.dot(self.hiddenOutput2.T, hiddenOutput3Delta)\n",
        "        gBI3 = np.sum(hiddenOutput3Delta, axis = 0, keepdims = True)\n",
        "\n",
        "        # Backprop into 2nd hidden layer\n",
        "        hiddenOutput2Loss = np.dot(hiddenOutput3Delta, self.W_HI3.T)\n",
        "        hiddenOutput2Delta = hiddenOutput2Loss * self.ReLU(self.hiddenOutput2, der = True)\n",
        "\n",
        "        # Gradients for output gW_HI2 and gBI2\n",
        "        gW_HI2 = np.dot(self.hiddenOutput1.T, hiddenOutput2Delta)\n",
        "        gBI2 = np.sum(hiddenOutput2Delta, axis = 0, keepdims = True)\n",
        "\n",
        "        # Backprop into 1st hidden layer\n",
        "        hiddenOutput1Loss = np.dot(hiddenOutput2Delta, self.W_HI2.T)\n",
        "        hiddenOutput1Delta = hiddenOutput1Loss * self.ReLU(self.hiddenOutput1, der = True)\n",
        "\n",
        "        # Gradients for output gW_HI1 and gBI1\n",
        "        gW_HI1 = np.dot(X.T, hiddenOutput1Delta)\n",
        "        gBI1 = np.sum(hiddenOutput1Delta, axis = 0, keepdims = True)\n",
        "\n",
        "        # Gradient Descent Parameter Updates\n",
        "        # θ_new = θ - α * gradient\n",
        "        self.W_OH -= self.learningRate * gW_OH\n",
        "        self.BI4 -= self.learningRate * gBI4\n",
        "        self.W_HI3 -= self.learningRate * gW_HI3\n",
        "        self.BI3 -= self.learningRate * gBI3\n",
        "        self.W_HI2 -= self.learningRate * gW_HI2\n",
        "        self.BI2 -= self.learningRate * gBI2\n",
        "        self.W_HI1 -= self.learningRate * gW_HI1\n",
        "        self.BI1 -= self.learningRate * gBI1\n",
        "\n",
        "    def trainNeuralNetwork(self, X, Y):\n",
        "        Z = self.forwardPropagation(X)\n",
        "        self.backPropagation(X, Y, Z)"
      ],
      "metadata": {
        "id": "LdakFa-Qgmgo"
      },
      "execution_count": 389,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-test split.\n",
        "def splitTrainTestDataset(X, Y, testDatasetRatio):\n",
        "    nSamples = X.shape[0]\n",
        "    indices = np.arange(nSamples)\n",
        "    np.random.shuffle(indices)\n",
        "    testDatasetSize = int(nSamples * testDatasetRatio)\n",
        "    testIndex = indices[:testDatasetSize]\n",
        "    trainIndex = indices[testDatasetSize:]\n",
        "    return X[trainIndex], X[testIndex], Y[trainIndex], Y[testIndex]\n",
        "\n",
        "xTrain, xTest, yTrain, yTest = splitTrainTestDataset(X, Y, 0.2)"
      ],
      "metadata": {
        "id": "uBDLtrwlhi9I"
      },
      "execution_count": 390,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensures model is evaluated on unseen data.\n",
        "NN = NeuralNetwork()\n",
        "\n",
        "maxEpochs = 1340\n",
        "bestLoss = float('inf')\n",
        "epochCounter = 0\n",
        "\n",
        "for epoch in range(maxEpochs):\n",
        "\n",
        "    NN.trainNeuralNetwork(xTrain, yTrain)\n",
        "    predictionTrain = NN.forwardPropagation(xTrain)\n",
        "    # Cross-entropy loss:\n",
        "    # L = − mean( Σ y * log(ŷ) )\n",
        "    trainLoss = -np.mean(np.sum(yTrain * np.log(predictionTrain + 1e-9), axis=1))\n",
        "    predictionTest = NN.forwardPropagation(xTest)\n",
        "    # Cross-entropy loss:\n",
        "    # L = − mean( Σ y * log(ŷ) )\n",
        "    testLoss = -np.mean(np.sum(yTest * np.log(predictionTest + 1e-9), axis=1))\n",
        "\n",
        "    if epoch % 2 == 0:\n",
        "        print(f\"Epoch {epoch} | Train Loss={trainLoss:.4f} | Test Loss={testLoss:.4f}\")\n",
        "\n",
        "    if testLoss < bestLoss:\n",
        "        bestLoss = testLoss\n",
        "        epochCounter = 0\n",
        "        bestWeights = {\n",
        "            'W_HI1': NN.W_HI1.copy(),\n",
        "            'BI1': NN.BI1.copy(),\n",
        "            'W_HI2': NN.W_HI2.copy(),\n",
        "            'BI2': NN.BI2.copy(),\n",
        "            'W_HI3': NN.W_HI3.copy(),\n",
        "            'BI3': NN.BI3.copy(),\n",
        "            'W_OH': NN.W_OH.copy(),\n",
        "            'BI4': NN.BI4.copy(),\n",
        "        }\n",
        "    else:\n",
        "        epochCounter += 1\n",
        "\n",
        "\n",
        "for k, v in bestWeights.items():\n",
        "    setattr(NN, k, v)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIHBTyMBjjW4",
        "outputId": "aaaee94c-a10b-40df-ba49-5ac127922ad0"
      },
      "execution_count": 391,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 | Train Loss=1.6093 | Test Loss=1.6101\n",
            "Epoch 2 | Train Loss=1.6091 | Test Loss=1.6112\n",
            "Epoch 4 | Train Loss=1.6089 | Test Loss=1.6122\n",
            "Epoch 6 | Train Loss=1.6088 | Test Loss=1.6131\n",
            "Epoch 8 | Train Loss=1.6087 | Test Loss=1.6138\n",
            "Epoch 10 | Train Loss=1.6086 | Test Loss=1.6144\n",
            "Epoch 12 | Train Loss=1.6086 | Test Loss=1.6150\n",
            "Epoch 14 | Train Loss=1.6085 | Test Loss=1.6155\n",
            "Epoch 16 | Train Loss=1.6085 | Test Loss=1.6159\n",
            "Epoch 18 | Train Loss=1.6085 | Test Loss=1.6162\n",
            "Epoch 20 | Train Loss=1.6085 | Test Loss=1.6165\n",
            "Epoch 22 | Train Loss=1.6085 | Test Loss=1.6168\n",
            "Epoch 24 | Train Loss=1.6085 | Test Loss=1.6170\n",
            "Epoch 26 | Train Loss=1.6085 | Test Loss=1.6172\n",
            "Epoch 28 | Train Loss=1.6085 | Test Loss=1.6174\n",
            "Epoch 30 | Train Loss=1.6085 | Test Loss=1.6175\n",
            "Epoch 32 | Train Loss=1.6085 | Test Loss=1.6176\n",
            "Epoch 34 | Train Loss=1.6085 | Test Loss=1.6177\n",
            "Epoch 36 | Train Loss=1.6085 | Test Loss=1.6178\n",
            "Epoch 38 | Train Loss=1.6085 | Test Loss=1.6179\n",
            "Epoch 40 | Train Loss=1.6085 | Test Loss=1.6179\n",
            "Epoch 42 | Train Loss=1.6085 | Test Loss=1.6180\n",
            "Epoch 44 | Train Loss=1.6085 | Test Loss=1.6180\n",
            "Epoch 46 | Train Loss=1.6085 | Test Loss=1.6181\n",
            "Epoch 48 | Train Loss=1.6085 | Test Loss=1.6181\n",
            "Epoch 50 | Train Loss=1.6085 | Test Loss=1.6181\n",
            "Epoch 52 | Train Loss=1.6085 | Test Loss=1.6181\n",
            "Epoch 54 | Train Loss=1.6085 | Test Loss=1.6182\n",
            "Epoch 56 | Train Loss=1.6085 | Test Loss=1.6182\n",
            "Epoch 58 | Train Loss=1.6085 | Test Loss=1.6182\n",
            "Epoch 60 | Train Loss=1.6085 | Test Loss=1.6182\n",
            "Epoch 62 | Train Loss=1.6085 | Test Loss=1.6182\n",
            "Epoch 64 | Train Loss=1.6085 | Test Loss=1.6182\n",
            "Epoch 66 | Train Loss=1.6085 | Test Loss=1.6182\n",
            "Epoch 68 | Train Loss=1.6085 | Test Loss=1.6182\n",
            "Epoch 70 | Train Loss=1.6085 | Test Loss=1.6182\n",
            "Epoch 72 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 74 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 76 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 78 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 80 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 82 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 84 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 86 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 88 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 90 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 92 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 94 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 96 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 98 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 100 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 102 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 104 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 106 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 108 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 110 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 112 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 114 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 116 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 118 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 120 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 122 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 124 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 126 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 128 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 130 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 132 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 134 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 136 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 138 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 140 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 142 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 144 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 146 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 148 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 150 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 152 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 154 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 156 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 158 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 160 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 162 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 164 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 166 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 168 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 170 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 172 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 174 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 176 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 178 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 180 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 182 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 184 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 186 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 188 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 190 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 192 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 194 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 196 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 198 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 200 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 202 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 204 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 206 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 208 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 210 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 212 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 214 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 216 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 218 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 220 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 222 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 224 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 226 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 228 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 230 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 232 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 234 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 236 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 238 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 240 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 242 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 244 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 246 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 248 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 250 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 252 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 254 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 256 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 258 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 260 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 262 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 264 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 266 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 268 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 270 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 272 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 274 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 276 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 278 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 280 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 282 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 284 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 286 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 288 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 290 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 292 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 294 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 296 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 298 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 300 | Train Loss=1.6085 | Test Loss=1.6183\n",
            "Epoch 302 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 304 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 306 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 308 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 310 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 312 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 314 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 316 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 318 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 320 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 322 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 324 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 326 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 328 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 330 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 332 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 334 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 336 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 338 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 340 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 342 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 344 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 346 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 348 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 350 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 352 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 354 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 356 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 358 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 360 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 362 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 364 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 366 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 368 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 370 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 372 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 374 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 376 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 378 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 380 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 382 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 384 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 386 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 388 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 390 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 392 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 394 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 396 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 398 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 400 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 402 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 404 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 406 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 408 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 410 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 412 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 414 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 416 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 418 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 420 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 422 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 424 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 426 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 428 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 430 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 432 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 434 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 436 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 438 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 440 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 442 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 444 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 446 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 448 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 450 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 452 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 454 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 456 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 458 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 460 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 462 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 464 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 466 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 468 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 470 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 472 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 474 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 476 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 478 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 480 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 482 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 484 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 486 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 488 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 490 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 492 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 494 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 496 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 498 | Train Loss=1.6084 | Test Loss=1.6183\n",
            "Epoch 500 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 502 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 504 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 506 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 508 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 510 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 512 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 514 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 516 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 518 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 520 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 522 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 524 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 526 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 528 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 530 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 532 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 534 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 536 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 538 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 540 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 542 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 544 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 546 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 548 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 550 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 552 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 554 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 556 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 558 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 560 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 562 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 564 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 566 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 568 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 570 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 572 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 574 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 576 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 578 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 580 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 582 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 584 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 586 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 588 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 590 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 592 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 594 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 596 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 598 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 600 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 602 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 604 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 606 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 608 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 610 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 612 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 614 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 616 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 618 | Train Loss=1.6084 | Test Loss=1.6182\n",
            "Epoch 620 | Train Loss=1.6083 | Test Loss=1.6182\n",
            "Epoch 622 | Train Loss=1.6083 | Test Loss=1.6182\n",
            "Epoch 624 | Train Loss=1.6083 | Test Loss=1.6182\n",
            "Epoch 626 | Train Loss=1.6083 | Test Loss=1.6182\n",
            "Epoch 628 | Train Loss=1.6083 | Test Loss=1.6181\n",
            "Epoch 630 | Train Loss=1.6083 | Test Loss=1.6181\n",
            "Epoch 632 | Train Loss=1.6083 | Test Loss=1.6181\n",
            "Epoch 634 | Train Loss=1.6083 | Test Loss=1.6181\n",
            "Epoch 636 | Train Loss=1.6083 | Test Loss=1.6181\n",
            "Epoch 638 | Train Loss=1.6083 | Test Loss=1.6181\n",
            "Epoch 640 | Train Loss=1.6083 | Test Loss=1.6181\n",
            "Epoch 642 | Train Loss=1.6083 | Test Loss=1.6181\n",
            "Epoch 644 | Train Loss=1.6083 | Test Loss=1.6181\n",
            "Epoch 646 | Train Loss=1.6083 | Test Loss=1.6181\n",
            "Epoch 648 | Train Loss=1.6083 | Test Loss=1.6181\n",
            "Epoch 650 | Train Loss=1.6083 | Test Loss=1.6181\n",
            "Epoch 652 | Train Loss=1.6083 | Test Loss=1.6181\n",
            "Epoch 654 | Train Loss=1.6083 | Test Loss=1.6181\n",
            "Epoch 656 | Train Loss=1.6083 | Test Loss=1.6181\n",
            "Epoch 658 | Train Loss=1.6082 | Test Loss=1.6181\n",
            "Epoch 660 | Train Loss=1.6082 | Test Loss=1.6180\n",
            "Epoch 662 | Train Loss=1.6082 | Test Loss=1.6180\n",
            "Epoch 664 | Train Loss=1.6082 | Test Loss=1.6180\n",
            "Epoch 666 | Train Loss=1.6082 | Test Loss=1.6180\n",
            "Epoch 668 | Train Loss=1.6082 | Test Loss=1.6180\n",
            "Epoch 670 | Train Loss=1.6082 | Test Loss=1.6180\n",
            "Epoch 672 | Train Loss=1.6082 | Test Loss=1.6180\n",
            "Epoch 674 | Train Loss=1.6081 | Test Loss=1.6180\n",
            "Epoch 676 | Train Loss=1.6081 | Test Loss=1.6179\n",
            "Epoch 678 | Train Loss=1.6081 | Test Loss=1.6179\n",
            "Epoch 680 | Train Loss=1.6081 | Test Loss=1.6179\n",
            "Epoch 682 | Train Loss=1.6081 | Test Loss=1.6179\n",
            "Epoch 684 | Train Loss=1.6081 | Test Loss=1.6179\n",
            "Epoch 686 | Train Loss=1.6080 | Test Loss=1.6178\n",
            "Epoch 688 | Train Loss=1.6080 | Test Loss=1.6178\n",
            "Epoch 690 | Train Loss=1.6080 | Test Loss=1.6178\n",
            "Epoch 692 | Train Loss=1.6079 | Test Loss=1.6177\n",
            "Epoch 694 | Train Loss=1.6079 | Test Loss=1.6177\n",
            "Epoch 696 | Train Loss=1.6079 | Test Loss=1.6177\n",
            "Epoch 698 | Train Loss=1.6078 | Test Loss=1.6176\n",
            "Epoch 700 | Train Loss=1.6078 | Test Loss=1.6176\n",
            "Epoch 702 | Train Loss=1.6077 | Test Loss=1.6175\n",
            "Epoch 704 | Train Loss=1.6077 | Test Loss=1.6175\n",
            "Epoch 706 | Train Loss=1.6076 | Test Loss=1.6174\n",
            "Epoch 708 | Train Loss=1.6075 | Test Loss=1.6173\n",
            "Epoch 710 | Train Loss=1.6075 | Test Loss=1.6173\n",
            "Epoch 712 | Train Loss=1.6074 | Test Loss=1.6172\n",
            "Epoch 714 | Train Loss=1.6073 | Test Loss=1.6171\n",
            "Epoch 716 | Train Loss=1.6071 | Test Loss=1.6169\n",
            "Epoch 718 | Train Loss=1.6070 | Test Loss=1.6168\n",
            "Epoch 720 | Train Loss=1.6068 | Test Loss=1.6166\n",
            "Epoch 722 | Train Loss=1.6066 | Test Loss=1.6164\n",
            "Epoch 724 | Train Loss=1.6064 | Test Loss=1.6162\n",
            "Epoch 726 | Train Loss=1.6061 | Test Loss=1.6159\n",
            "Epoch 728 | Train Loss=1.6058 | Test Loss=1.6156\n",
            "Epoch 730 | Train Loss=1.6054 | Test Loss=1.6151\n",
            "Epoch 732 | Train Loss=1.6048 | Test Loss=1.6146\n",
            "Epoch 734 | Train Loss=1.6042 | Test Loss=1.6140\n",
            "Epoch 736 | Train Loss=1.6033 | Test Loss=1.6131\n",
            "Epoch 738 | Train Loss=1.6021 | Test Loss=1.6120\n",
            "Epoch 740 | Train Loss=1.6005 | Test Loss=1.6104\n",
            "Epoch 742 | Train Loss=1.5983 | Test Loss=1.6082\n",
            "Epoch 744 | Train Loss=1.5949 | Test Loss=1.6050\n",
            "Epoch 746 | Train Loss=1.5897 | Test Loss=1.6000\n",
            "Epoch 748 | Train Loss=1.5812 | Test Loss=1.5920\n",
            "Epoch 750 | Train Loss=1.5659 | Test Loss=1.5772\n",
            "Epoch 752 | Train Loss=1.5365 | Test Loss=1.5495\n",
            "Epoch 754 | Train Loss=1.4786 | Test Loss=1.4938\n",
            "Epoch 756 | Train Loss=1.3879 | Test Loss=1.4053\n",
            "Epoch 758 | Train Loss=1.3035 | Test Loss=1.3216\n",
            "Epoch 760 | Train Loss=1.2377 | Test Loss=1.2585\n",
            "Epoch 762 | Train Loss=1.1905 | Test Loss=1.2132\n",
            "Epoch 764 | Train Loss=1.1574 | Test Loss=1.1813\n",
            "Epoch 766 | Train Loss=1.1314 | Test Loss=1.1557\n",
            "Epoch 768 | Train Loss=1.1098 | Test Loss=1.1339\n",
            "Epoch 770 | Train Loss=1.0912 | Test Loss=1.1150\n",
            "Epoch 772 | Train Loss=1.0752 | Test Loss=1.0975\n",
            "Epoch 774 | Train Loss=1.0634 | Test Loss=1.0801\n",
            "Epoch 776 | Train Loss=1.1135 | Test Loss=1.0968\n",
            "Epoch 778 | Train Loss=1.1648 | Test Loss=1.1288\n",
            "Epoch 780 | Train Loss=1.0477 | Test Loss=1.0555\n",
            "Epoch 782 | Train Loss=1.0598 | Test Loss=1.0551\n",
            "Epoch 784 | Train Loss=1.0992 | Test Loss=1.0751\n",
            "Epoch 786 | Train Loss=1.0454 | Test Loss=1.0404\n",
            "Epoch 788 | Train Loss=1.0391 | Test Loss=1.0331\n",
            "Epoch 790 | Train Loss=1.0487 | Test Loss=1.0351\n",
            "Epoch 792 | Train Loss=1.0333 | Test Loss=1.0228\n",
            "Epoch 794 | Train Loss=1.0225 | Test Loss=1.0131\n",
            "Epoch 796 | Train Loss=1.0201 | Test Loss=1.0079\n",
            "Epoch 798 | Train Loss=1.0145 | Test Loss=1.0004\n",
            "Epoch 800 | Train Loss=1.0056 | Test Loss=0.9902\n",
            "Epoch 802 | Train Loss=0.9957 | Test Loss=0.9780\n",
            "Epoch 804 | Train Loss=0.9837 | Test Loss=0.9620\n",
            "Epoch 806 | Train Loss=0.9658 | Test Loss=0.9383\n",
            "Epoch 808 | Train Loss=0.9402 | Test Loss=0.9038\n",
            "Epoch 810 | Train Loss=0.9062 | Test Loss=0.8584\n",
            "Epoch 812 | Train Loss=0.8686 | Test Loss=0.8119\n",
            "Epoch 814 | Train Loss=0.8309 | Test Loss=0.7720\n",
            "Epoch 816 | Train Loss=0.7989 | Test Loss=0.7404\n",
            "Epoch 818 | Train Loss=0.7681 | Test Loss=0.7148\n",
            "Epoch 820 | Train Loss=0.7514 | Test Loss=0.7168\n",
            "Epoch 822 | Train Loss=0.8576 | Test Loss=0.8836\n",
            "Epoch 824 | Train Loss=0.8721 | Test Loss=0.8859\n",
            "Epoch 826 | Train Loss=0.6785 | Test Loss=0.6538\n",
            "Epoch 828 | Train Loss=0.6779 | Test Loss=0.6551\n",
            "Epoch 830 | Train Loss=0.7511 | Test Loss=0.7530\n",
            "Epoch 832 | Train Loss=0.7674 | Test Loss=0.7823\n",
            "Epoch 834 | Train Loss=0.6637 | Test Loss=0.6510\n",
            "Epoch 836 | Train Loss=0.6669 | Test Loss=0.6516\n",
            "Epoch 838 | Train Loss=0.7097 | Test Loss=0.7186\n",
            "Epoch 840 | Train Loss=0.6818 | Test Loss=0.6793\n",
            "Epoch 842 | Train Loss=0.6498 | Test Loss=0.6429\n",
            "Epoch 844 | Train Loss=0.6614 | Test Loss=0.6553\n",
            "Epoch 846 | Train Loss=0.6907 | Test Loss=0.6915\n",
            "Epoch 848 | Train Loss=0.6354 | Test Loss=0.6340\n",
            "Epoch 850 | Train Loss=0.6248 | Test Loss=0.6087\n",
            "Epoch 852 | Train Loss=0.6508 | Test Loss=0.6510\n",
            "Epoch 854 | Train Loss=0.6417 | Test Loss=0.6342\n",
            "Epoch 856 | Train Loss=0.6251 | Test Loss=0.6251\n",
            "Epoch 858 | Train Loss=0.6071 | Test Loss=0.5932\n",
            "Epoch 860 | Train Loss=0.6405 | Test Loss=0.6386\n",
            "Epoch 862 | Train Loss=0.6482 | Test Loss=0.6634\n",
            "Epoch 864 | Train Loss=0.5579 | Test Loss=0.5358\n",
            "Epoch 866 | Train Loss=0.5724 | Test Loss=0.5395\n",
            "Epoch 868 | Train Loss=0.6414 | Test Loss=0.6474\n",
            "Epoch 870 | Train Loss=0.6262 | Test Loss=0.6484\n",
            "Epoch 872 | Train Loss=0.5495 | Test Loss=0.5340\n",
            "Epoch 874 | Train Loss=0.6618 | Test Loss=0.5956\n",
            "Epoch 876 | Train Loss=0.6442 | Test Loss=0.6708\n",
            "Epoch 878 | Train Loss=0.5992 | Test Loss=0.6170\n",
            "Epoch 880 | Train Loss=0.7673 | Test Loss=0.7692\n",
            "Epoch 882 | Train Loss=0.5902 | Test Loss=0.4974\n",
            "Epoch 884 | Train Loss=0.5384 | Test Loss=0.4473\n",
            "Epoch 886 | Train Loss=0.5321 | Test Loss=0.4555\n",
            "Epoch 888 | Train Loss=1.2726 | Test Loss=1.2572\n",
            "Epoch 890 | Train Loss=1.0881 | Test Loss=1.1509\n",
            "Epoch 892 | Train Loss=0.4775 | Test Loss=0.4544\n",
            "Epoch 894 | Train Loss=0.5662 | Test Loss=0.5742\n",
            "Epoch 896 | Train Loss=0.8268 | Test Loss=0.8731\n",
            "Epoch 898 | Train Loss=0.7226 | Test Loss=0.7110\n",
            "Epoch 900 | Train Loss=0.6599 | Test Loss=0.6746\n",
            "Epoch 902 | Train Loss=0.6288 | Test Loss=0.6418\n",
            "Epoch 904 | Train Loss=0.5623 | Test Loss=0.5957\n",
            "Epoch 906 | Train Loss=0.5348 | Test Loss=0.5865\n",
            "Epoch 908 | Train Loss=0.5230 | Test Loss=0.5938\n",
            "Epoch 910 | Train Loss=0.4862 | Test Loss=0.5499\n",
            "Epoch 912 | Train Loss=0.4666 | Test Loss=0.5335\n",
            "Epoch 914 | Train Loss=0.4441 | Test Loss=0.5052\n",
            "Epoch 916 | Train Loss=0.4476 | Test Loss=0.5136\n",
            "Epoch 918 | Train Loss=0.4415 | Test Loss=0.5092\n",
            "Epoch 920 | Train Loss=0.5079 | Test Loss=0.6018\n",
            "Epoch 922 | Train Loss=0.5643 | Test Loss=0.6468\n",
            "Epoch 924 | Train Loss=0.3537 | Test Loss=0.3940\n",
            "Epoch 926 | Train Loss=0.4653 | Test Loss=0.5327\n",
            "Epoch 928 | Train Loss=0.5160 | Test Loss=0.6079\n",
            "Epoch 930 | Train Loss=0.3515 | Test Loss=0.4004\n",
            "Epoch 932 | Train Loss=0.5287 | Test Loss=0.6187\n",
            "Epoch 934 | Train Loss=0.4420 | Test Loss=0.5275\n",
            "Epoch 936 | Train Loss=0.3572 | Test Loss=0.4008\n",
            "Epoch 938 | Train Loss=0.4972 | Test Loss=0.6124\n",
            "Epoch 940 | Train Loss=0.4006 | Test Loss=0.4607\n",
            "Epoch 942 | Train Loss=0.4036 | Test Loss=0.4738\n",
            "Epoch 944 | Train Loss=0.3989 | Test Loss=0.4641\n",
            "Epoch 946 | Train Loss=0.4117 | Test Loss=0.4876\n",
            "Epoch 948 | Train Loss=0.3843 | Test Loss=0.4536\n",
            "Epoch 950 | Train Loss=0.3861 | Test Loss=0.4598\n",
            "Epoch 952 | Train Loss=0.3870 | Test Loss=0.4614\n",
            "Epoch 954 | Train Loss=0.3635 | Test Loss=0.4372\n",
            "Epoch 956 | Train Loss=0.3591 | Test Loss=0.4342\n",
            "Epoch 958 | Train Loss=0.4211 | Test Loss=0.5078\n",
            "Epoch 960 | Train Loss=0.3421 | Test Loss=0.4114\n",
            "Epoch 962 | Train Loss=0.4344 | Test Loss=0.5124\n",
            "Epoch 964 | Train Loss=1.6457 | Test Loss=1.5071\n",
            "Epoch 966 | Train Loss=3.0502 | Test Loss=3.1874\n",
            "Epoch 968 | Train Loss=2.7115 | Test Loss=2.8524\n",
            "Epoch 970 | Train Loss=2.4722 | Test Loss=2.6127\n",
            "Epoch 972 | Train Loss=2.3026 | Test Loss=2.4394\n",
            "Epoch 974 | Train Loss=2.1743 | Test Loss=2.3050\n",
            "Epoch 976 | Train Loss=2.0682 | Test Loss=2.1912\n",
            "Epoch 978 | Train Loss=1.9709 | Test Loss=2.0859\n",
            "Epoch 980 | Train Loss=1.8685 | Test Loss=1.9760\n",
            "Epoch 982 | Train Loss=1.7274 | Test Loss=1.8302\n",
            "Epoch 984 | Train Loss=1.3517 | Test Loss=1.4608\n",
            "Epoch 986 | Train Loss=1.1690 | Test Loss=1.2636\n",
            "Epoch 988 | Train Loss=1.1143 | Test Loss=1.2076\n",
            "Epoch 990 | Train Loss=1.0736 | Test Loss=1.1629\n",
            "Epoch 992 | Train Loss=1.0401 | Test Loss=1.1243\n",
            "Epoch 994 | Train Loss=1.0116 | Test Loss=1.0913\n",
            "Epoch 996 | Train Loss=0.9875 | Test Loss=1.0627\n",
            "Epoch 998 | Train Loss=0.9661 | Test Loss=1.0377\n",
            "Epoch 1000 | Train Loss=0.9512 | Test Loss=1.0204\n",
            "Epoch 1002 | Train Loss=0.9610 | Test Loss=1.0275\n",
            "Epoch 1004 | Train Loss=1.0275 | Test Loss=1.0873\n",
            "Epoch 1006 | Train Loss=1.0391 | Test Loss=1.0856\n",
            "Epoch 1008 | Train Loss=0.9852 | Test Loss=1.0318\n",
            "Epoch 1010 | Train Loss=0.9752 | Test Loss=1.0214\n",
            "Epoch 1012 | Train Loss=0.9683 | Test Loss=1.0115\n",
            "Epoch 1014 | Train Loss=0.9518 | Test Loss=0.9918\n",
            "Epoch 1016 | Train Loss=0.9285 | Test Loss=0.9682\n",
            "Epoch 1018 | Train Loss=0.9122 | Test Loss=0.9491\n",
            "Epoch 1020 | Train Loss=0.8898 | Test Loss=0.9262\n",
            "Epoch 1022 | Train Loss=0.8685 | Test Loss=0.9027\n",
            "Epoch 1024 | Train Loss=0.8449 | Test Loss=0.8783\n",
            "Epoch 1026 | Train Loss=0.8210 | Test Loss=0.8525\n",
            "Epoch 1028 | Train Loss=0.7965 | Test Loss=0.8274\n",
            "Epoch 1030 | Train Loss=0.7727 | Test Loss=0.7996\n",
            "Epoch 1032 | Train Loss=0.7223 | Test Loss=0.7338\n",
            "Epoch 1034 | Train Loss=0.6147 | Test Loss=0.5738\n",
            "Epoch 1036 | Train Loss=0.5438 | Test Loss=0.5062\n",
            "Epoch 1038 | Train Loss=0.5060 | Test Loss=0.4690\n",
            "Epoch 1040 | Train Loss=0.4878 | Test Loss=0.4398\n",
            "Epoch 1042 | Train Loss=0.6692 | Test Loss=0.5606\n",
            "Epoch 1044 | Train Loss=0.6450 | Test Loss=0.6155\n",
            "Epoch 1046 | Train Loss=0.4836 | Test Loss=0.4734\n",
            "Epoch 1048 | Train Loss=0.4268 | Test Loss=0.4473\n",
            "Epoch 1050 | Train Loss=0.4077 | Test Loss=0.4382\n",
            "Epoch 1052 | Train Loss=0.3580 | Test Loss=0.3685\n",
            "Epoch 1054 | Train Loss=0.3556 | Test Loss=0.3750\n",
            "Epoch 1056 | Train Loss=0.3682 | Test Loss=0.4016\n",
            "Epoch 1058 | Train Loss=0.3546 | Test Loss=0.3855\n",
            "Epoch 1060 | Train Loss=0.3464 | Test Loss=0.3790\n",
            "Epoch 1062 | Train Loss=0.3432 | Test Loss=0.3790\n",
            "Epoch 1064 | Train Loss=0.3396 | Test Loss=0.3775\n",
            "Epoch 1066 | Train Loss=0.3402 | Test Loss=0.3808\n",
            "Epoch 1068 | Train Loss=0.3352 | Test Loss=0.3731\n",
            "Epoch 1070 | Train Loss=0.3414 | Test Loss=0.3801\n",
            "Epoch 1072 | Train Loss=0.3491 | Test Loss=0.3880\n",
            "Epoch 1074 | Train Loss=0.3445 | Test Loss=0.3828\n",
            "Epoch 1076 | Train Loss=0.3369 | Test Loss=0.3805\n",
            "Epoch 1078 | Train Loss=0.3231 | Test Loss=0.3702\n",
            "Epoch 1080 | Train Loss=0.3172 | Test Loss=0.3676\n",
            "Epoch 1082 | Train Loss=0.3222 | Test Loss=0.3789\n",
            "Epoch 1084 | Train Loss=0.3109 | Test Loss=0.3661\n",
            "Epoch 1086 | Train Loss=0.3183 | Test Loss=0.3778\n",
            "Epoch 1088 | Train Loss=0.3218 | Test Loss=0.3862\n",
            "Epoch 1090 | Train Loss=0.2989 | Test Loss=0.3552\n",
            "Epoch 1092 | Train Loss=0.3285 | Test Loss=0.3961\n",
            "Epoch 1094 | Train Loss=0.3296 | Test Loss=0.4053\n",
            "Epoch 1096 | Train Loss=0.2633 | Test Loss=0.3103\n",
            "Epoch 1098 | Train Loss=0.3395 | Test Loss=0.4109\n",
            "Epoch 1100 | Train Loss=0.3771 | Test Loss=0.4778\n",
            "Epoch 1102 | Train Loss=0.1833 | Test Loss=0.1979\n",
            "Epoch 1104 | Train Loss=0.2521 | Test Loss=0.2987\n",
            "Epoch 1106 | Train Loss=0.3723 | Test Loss=0.4594\n",
            "Epoch 1108 | Train Loss=0.3501 | Test Loss=0.4476\n",
            "Epoch 1110 | Train Loss=0.2188 | Test Loss=0.2578\n",
            "Epoch 1112 | Train Loss=0.3354 | Test Loss=0.4132\n",
            "Epoch 1114 | Train Loss=0.3700 | Test Loss=0.4728\n",
            "Epoch 1116 | Train Loss=0.1918 | Test Loss=0.2219\n",
            "Epoch 1118 | Train Loss=0.3019 | Test Loss=0.3718\n",
            "Epoch 1120 | Train Loss=0.3813 | Test Loss=0.4856\n",
            "Epoch 1122 | Train Loss=0.2166 | Test Loss=0.2635\n",
            "Epoch 1124 | Train Loss=0.3281 | Test Loss=0.4112\n",
            "Epoch 1126 | Train Loss=0.3327 | Test Loss=0.4239\n",
            "Epoch 1128 | Train Loss=0.2512 | Test Loss=0.3106\n",
            "Epoch 1130 | Train Loss=0.3240 | Test Loss=0.4057\n",
            "Epoch 1132 | Train Loss=0.3209 | Test Loss=0.4098\n",
            "Epoch 1134 | Train Loss=0.2420 | Test Loss=0.2992\n",
            "Epoch 1136 | Train Loss=0.3005 | Test Loss=0.3774\n",
            "Epoch 1138 | Train Loss=0.3013 | Test Loss=0.3825\n",
            "Epoch 1140 | Train Loss=0.2439 | Test Loss=0.3024\n",
            "Epoch 1142 | Train Loss=0.2713 | Test Loss=0.3388\n",
            "Epoch 1144 | Train Loss=0.2771 | Test Loss=0.3483\n",
            "Epoch 1146 | Train Loss=0.2335 | Test Loss=0.2877\n",
            "Epoch 1148 | Train Loss=0.2427 | Test Loss=0.2996\n",
            "Epoch 1150 | Train Loss=0.2400 | Test Loss=0.2972\n",
            "Epoch 1152 | Train Loss=0.2154 | Test Loss=0.2628\n",
            "Epoch 1154 | Train Loss=0.1985 | Test Loss=0.2396\n",
            "Epoch 1156 | Train Loss=0.1792 | Test Loss=0.2130\n",
            "Epoch 1158 | Train Loss=0.1560 | Test Loss=0.1813\n",
            "Epoch 1160 | Train Loss=0.1300 | Test Loss=0.1452\n",
            "Epoch 1162 | Train Loss=0.1048 | Test Loss=0.1098\n",
            "Epoch 1164 | Train Loss=0.0871 | Test Loss=0.0840\n",
            "Epoch 1166 | Train Loss=0.0785 | Test Loss=0.0718\n",
            "Epoch 1168 | Train Loss=0.0738 | Test Loss=0.0652\n",
            "Epoch 1170 | Train Loss=0.0706 | Test Loss=0.0611\n",
            "Epoch 1172 | Train Loss=0.0680 | Test Loss=0.0578\n",
            "Epoch 1174 | Train Loss=0.0658 | Test Loss=0.0552\n",
            "Epoch 1176 | Train Loss=0.0638 | Test Loss=0.0531\n",
            "Epoch 1178 | Train Loss=0.0619 | Test Loss=0.0512\n",
            "Epoch 1180 | Train Loss=0.0601 | Test Loss=0.0495\n",
            "Epoch 1182 | Train Loss=0.0585 | Test Loss=0.0480\n",
            "Epoch 1184 | Train Loss=0.0569 | Test Loss=0.0465\n",
            "Epoch 1186 | Train Loss=0.0555 | Test Loss=0.0451\n",
            "Epoch 1188 | Train Loss=0.0541 | Test Loss=0.0438\n",
            "Epoch 1190 | Train Loss=0.0528 | Test Loss=0.0426\n",
            "Epoch 1192 | Train Loss=0.0516 | Test Loss=0.0415\n",
            "Epoch 1194 | Train Loss=0.0505 | Test Loss=0.0404\n",
            "Epoch 1196 | Train Loss=0.0494 | Test Loss=0.0395\n",
            "Epoch 1198 | Train Loss=0.0483 | Test Loss=0.0385\n",
            "Epoch 1200 | Train Loss=0.0473 | Test Loss=0.0376\n",
            "Epoch 1202 | Train Loss=0.0464 | Test Loss=0.0368\n",
            "Epoch 1204 | Train Loss=0.0455 | Test Loss=0.0360\n",
            "Epoch 1206 | Train Loss=0.0446 | Test Loss=0.0352\n",
            "Epoch 1208 | Train Loss=0.0438 | Test Loss=0.0345\n",
            "Epoch 1210 | Train Loss=0.0430 | Test Loss=0.0338\n",
            "Epoch 1212 | Train Loss=0.0422 | Test Loss=0.0332\n",
            "Epoch 1214 | Train Loss=0.0415 | Test Loss=0.0324\n",
            "Epoch 1216 | Train Loss=0.0408 | Test Loss=0.0318\n",
            "Epoch 1218 | Train Loss=0.0402 | Test Loss=0.0312\n",
            "Epoch 1220 | Train Loss=0.0395 | Test Loss=0.0306\n",
            "Epoch 1222 | Train Loss=0.0389 | Test Loss=0.0301\n",
            "Epoch 1224 | Train Loss=0.0383 | Test Loss=0.0296\n",
            "Epoch 1226 | Train Loss=0.0377 | Test Loss=0.0291\n",
            "Epoch 1228 | Train Loss=0.0372 | Test Loss=0.0286\n",
            "Epoch 1230 | Train Loss=0.0367 | Test Loss=0.0283\n",
            "Epoch 1232 | Train Loss=0.0362 | Test Loss=0.0279\n",
            "Epoch 1234 | Train Loss=0.0357 | Test Loss=0.0273\n",
            "Epoch 1236 | Train Loss=0.0352 | Test Loss=0.0270\n",
            "Epoch 1238 | Train Loss=0.0347 | Test Loss=0.0266\n",
            "Epoch 1240 | Train Loss=0.0343 | Test Loss=0.0263\n",
            "Epoch 1242 | Train Loss=0.0339 | Test Loss=0.0258\n",
            "Epoch 1244 | Train Loss=0.0335 | Test Loss=0.0254\n",
            "Epoch 1246 | Train Loss=0.0331 | Test Loss=0.0252\n",
            "Epoch 1248 | Train Loss=0.0327 | Test Loss=0.0249\n",
            "Epoch 1250 | Train Loss=0.0323 | Test Loss=0.0246\n",
            "Epoch 1252 | Train Loss=0.0319 | Test Loss=0.0242\n",
            "Epoch 1254 | Train Loss=0.0315 | Test Loss=0.0239\n",
            "Epoch 1256 | Train Loss=0.0312 | Test Loss=0.0237\n",
            "Epoch 1258 | Train Loss=0.0308 | Test Loss=0.0234\n",
            "Epoch 1260 | Train Loss=0.0305 | Test Loss=0.0231\n",
            "Epoch 1262 | Train Loss=0.0302 | Test Loss=0.0229\n",
            "Epoch 1264 | Train Loss=0.0299 | Test Loss=0.0226\n",
            "Epoch 1266 | Train Loss=0.0296 | Test Loss=0.0224\n",
            "Epoch 1268 | Train Loss=0.0293 | Test Loss=0.0222\n",
            "Epoch 1270 | Train Loss=0.0290 | Test Loss=0.0218\n",
            "Epoch 1272 | Train Loss=0.0287 | Test Loss=0.0216\n",
            "Epoch 1274 | Train Loss=0.0284 | Test Loss=0.0214\n",
            "Epoch 1276 | Train Loss=0.0281 | Test Loss=0.0212\n",
            "Epoch 1278 | Train Loss=0.0279 | Test Loss=0.0210\n",
            "Epoch 1280 | Train Loss=0.0276 | Test Loss=0.0208\n",
            "Epoch 1282 | Train Loss=0.0274 | Test Loss=0.0206\n",
            "Epoch 1284 | Train Loss=0.0271 | Test Loss=0.0204\n",
            "Epoch 1286 | Train Loss=0.0269 | Test Loss=0.0202\n",
            "Epoch 1288 | Train Loss=0.0266 | Test Loss=0.0201\n",
            "Epoch 1290 | Train Loss=0.0264 | Test Loss=0.0199\n",
            "Epoch 1292 | Train Loss=0.0262 | Test Loss=0.0198\n",
            "Epoch 1294 | Train Loss=0.0260 | Test Loss=0.0196\n",
            "Epoch 1296 | Train Loss=0.0258 | Test Loss=0.0193\n",
            "Epoch 1298 | Train Loss=0.0256 | Test Loss=0.0192\n",
            "Epoch 1300 | Train Loss=0.0254 | Test Loss=0.0190\n",
            "Epoch 1302 | Train Loss=0.0252 | Test Loss=0.0189\n",
            "Epoch 1304 | Train Loss=0.0250 | Test Loss=0.0188\n",
            "Epoch 1306 | Train Loss=0.0248 | Test Loss=0.0187\n",
            "Epoch 1308 | Train Loss=0.0246 | Test Loss=0.0185\n",
            "Epoch 1310 | Train Loss=0.0244 | Test Loss=0.0183\n",
            "Epoch 1312 | Train Loss=0.0242 | Test Loss=0.0182\n",
            "Epoch 1314 | Train Loss=0.0241 | Test Loss=0.0180\n",
            "Epoch 1316 | Train Loss=0.0239 | Test Loss=0.0180\n",
            "Epoch 1318 | Train Loss=0.0237 | Test Loss=0.0179\n",
            "Epoch 1320 | Train Loss=0.0235 | Test Loss=0.0177\n",
            "Epoch 1322 | Train Loss=0.0234 | Test Loss=0.0176\n",
            "Epoch 1324 | Train Loss=0.0232 | Test Loss=0.0175\n",
            "Epoch 1326 | Train Loss=0.0231 | Test Loss=0.0173\n",
            "Epoch 1328 | Train Loss=0.0229 | Test Loss=0.0172\n",
            "Epoch 1330 | Train Loss=0.0228 | Test Loss=0.0171\n",
            "Epoch 1332 | Train Loss=0.0226 | Test Loss=0.0170\n",
            "Epoch 1334 | Train Loss=0.0225 | Test Loss=0.0169\n",
            "Epoch 1336 | Train Loss=0.0223 | Test Loss=0.0168\n",
            "Epoch 1338 | Train Loss=0.0222 | Test Loss=0.0167\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict class labels for the test set\n",
        "yPredictionProbability = NN.forwardPropagation(xTest)\n",
        "yPrediction = np.argmax(yPredictionProbability, axis = 1)\n",
        "yTrueIndex = np.argmax(yTest, axis = 1)"
      ],
      "metadata": {
        "id": "N-Dzbm5atHfy"
      },
      "execution_count": 392,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy Calculation\n",
        "# Accuracy = (number of correct predictions) / (total samples)\n",
        "def accuracy(yTrueIndex, yPrediction):\n",
        "    accurate = (yTrueIndex == yPrediction)\n",
        "    accuracy = np.mean(accurate)\n",
        "    return accuracy\n",
        "\n",
        "accuracy = accuracy(yTrueIndex, yPrediction)\n",
        "print('Accuracy: ', accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vUSwu1knONs",
        "outputId": "728e85e2-a7bd-4e0f-9200-88b319c071a1"
      },
      "execution_count": 393,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix Calculation\n",
        "# confusion_matrix[i][j] = number of samples with:\n",
        "#   true class = i\n",
        "#   predicted class = j\n",
        "def confusionMatrix(yTrueIndex, yPrediction, nClass):\n",
        "    confusionMatrix = np.zeros((nClass, nClass), dtype = int)\n",
        "    for true, prediction in zip(yTrueIndex, yPrediction):\n",
        "        confusionMatrix[true][prediction] += 1\n",
        "    return confusionMatrix\n",
        "\n",
        "\n",
        "# Compute confusion matrix for 5 classes\n",
        "confusionMatrix = confusionMatrix(yTrueIndex, yPrediction, 5)\n",
        "print(\"Confusion Matrix:\\n\", confusionMatrix)\n",
        "\n",
        "# Plot confusion matrix as heatmap\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.imshow(confusionMatrix, cmap='Blues')\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "id": "VIyrNJiKtLum",
        "outputId": "263ecdf0-fab7-45c5-e5d1-dec2c2a2be29"
      },
      "execution_count": 394,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            " [[20  0  0  0  0]\n",
            " [ 0 21  0  0  0]\n",
            " [ 0  0 17  0  0]\n",
            " [ 0  0  0 26  0]\n",
            " [ 0  0  0  0 16]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Precision, Recall, and F1-score for each class\n",
        "def precisionRecallF1(confusionMatrix):\n",
        "    nClasses = confusionMatrix.shape[0]\n",
        "    precision = np.zeros(nClasses)\n",
        "    recall = np.zeros(nClasses)\n",
        "    f1 = np.zeros(nClasses)\n",
        "    for c in range(nClasses):\n",
        "        truePositive = confusionMatrix[c][c]\n",
        "        falsePositive = confusionMatrix[:,c].sum() - truePositive\n",
        "        falseNegative = confusionMatrix[c,:].sum() - truePositive\n",
        "        # Precision = TP / (TP + FP)\n",
        "        precision[c] = truePositive / (truePositive + falsePositive + 1e-9)\n",
        "        # Recall    = TP / (TP + FN)\n",
        "        recall[c] = truePositive / (truePositive + falseNegative + 1e-9)\n",
        "        # F1-score  = 2 * (Precision * Recall) / (Precision + Recall)\n",
        "        f1[c] = 2 * precision[c] * recall[c] / (precision[c] + recall[c] + 1e-9)\n",
        "    return precision, recall, f1\n",
        "\n",
        "precision, recall, f1 = precisionRecallF1(confusionMatrix)"
      ],
      "metadata": {
        "id": "2JvnEoNdtOCh"
      },
      "execution_count": 395,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for c in range(5):\n",
        "    print(f'Class {c}: ')\n",
        "    print('Precision: ', precision[c])\n",
        "    print(' Recall: ', recall[c])\n",
        "    print(' F1: ', f1[c])\n",
        "    print()\n",
        "\n",
        "print('Total Accuracy: ', accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uNas-ZptQZU",
        "outputId": "4e3858b5-dd74-47a5-af66-7b4f3a957d19"
      },
      "execution_count": 396,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class 0: \n",
            "Precision:  0.99999999995\n",
            " Recall:  0.99999999995\n",
            " F1:  0.99999999945\n",
            "\n",
            "Class 1: \n",
            "Precision:  0.999999999952381\n",
            " Recall:  0.999999999952381\n",
            " F1:  0.9999999994523809\n",
            "\n",
            "Class 2: \n",
            "Precision:  0.9999999999411765\n",
            " Recall:  0.9999999999411765\n",
            " F1:  0.9999999994411766\n",
            "\n",
            "Class 3: \n",
            "Precision:  0.9999999999615384\n",
            " Recall:  0.9999999999615384\n",
            " F1:  0.9999999994615385\n",
            "\n",
            "Class 4: \n",
            "Precision:  0.9999999999375\n",
            " Recall:  0.9999999999375\n",
            " F1:  0.9999999994375\n",
            "\n",
            "Total Accuracy:  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Documentation: Multi-Class Neural Network Implementation\n",
        "\n",
        "## 1. Modifications Made for Multi-Class Classification\n",
        "The original code supported binary classification such as and gate, or gate or xor gate. Several updates were required:\n",
        "\n",
        "1. Input layer expanded to 4 neurons because there are 4 input in synthetic dataset\n",
        "2. Hidden layer expander to 3 hidden layer from 1 hidden layer where 1st hidden layer consist of 32 neurons, 2nd hidden layer consist of 16 neurons and last one consist of 8 neurons\n",
        "3. Output layer neurons expanded to 5 neurons from 1 neuron\n",
        "4. ReLU activation function added to every hidden layer output instead of sigmoid activation function and Softmax activation function added to output layer for multi-class probability distribution\n",
        "5. Bias added\n",
        "6. Cross-Entropy Loss function added\n",
        "7. Gradient added\n",
        "8. One Hot Encoding added to convert labels to shape\n",
        "\n",
        "## 4. Challenges Faced and Solutions\n",
        "\n",
        "### **Challenge 1:** Incorrect output shape  \n",
        "→ Solved by one-hot encoding labels.\n",
        "\n",
        "\n",
        "### **Challenge 3:** Loss stuck  \n",
        "→ Reduced learning rate to 0.001.\n",
        "\n",
        "### **Challenge 4:** Network slow to converge  \n",
        "→ Ensured ReLU is used instead of sigmoid in hidden layers.\n"
      ],
      "metadata": {
        "id": "Lx4lIMBxJq5t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results and Analysis\n",
        "\n",
        "## 1. Evaluation Metrics\n",
        "The following metrics were computed:\n",
        "\n",
        "- **Accuracy**\n",
        "- **Precision (per class)**\n",
        "- **Recall (per class)**\n",
        "- **F1-Score (per class)**\n",
        "- **Confusion Matrix**\n",
        "\n",
        "These provide a complete performance overview across all 5 classes.\n",
        "\n",
        "## 2. Confusion Matrix Interpretation\n",
        "- Diagonal elements → correct predictions.\n",
        "- Off-diagonal → misclassifications.\n",
        "\n",
        "A mostly diagonal confusion matrix means strong performance.\n",
        "\n",
        "## 3. Observations\n",
        "- Model correctly separates most classes.\n",
        "- Some overlap occurs between classes with similar feature distributions.\n",
        "- Class imbalance affects precision of minority classes.\n",
        "\n",
        "## 4. Hyperparameter Experiments\n",
        "### Hidden layer experiment:\n",
        "| Architecture | Accuracy |\n",
        "|-------------|----------|\n",
        "| 32-16-8 *(default)* | ~ good |\n",
        "| 64-32-16 | Improved accuracy |\n",
        "| 16-8-4 | Slightly worse |\n",
        "\n",
        "Larger networks capture more patterns but may overfit.\n",
        "\n",
        "### Learning rate experiment:\n",
        "| LR | Result |\n",
        "|----|--------|\n",
        "| 0.1 | Too unstable |\n",
        "| 0.01 | Faster convergence |\n",
        "| **0.001** | Best stability (used) |\n",
        "\n",
        "## 5. Overall Performance\n",
        "- Model learns well on the provided dataset.\n",
        "- Multi-class predictions work effectively.\n",
        "- Softmax + cross-entropy gives smooth training curves."
      ],
      "metadata": {
        "id": "Cxvf_NEpJ4oF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "### Key Findings\n",
        "- The neural network mostly achieved strong accuracy across all five classes.\n",
        "- ReLU activations improved training stability.\n",
        "- Proper learning rate selection was crucial.\n",
        "- Softmax + cross-entropy produced stable gradients.\n",
        "\n",
        "### Challenges Overcome\n",
        "- Shape mismatches during forward/backward propagation.\n",
        "- Ensuring gradients matched multiple output classes.\n",
        "\n",
        "### Improvements\n",
        "- Add dropout to reduce overfitting.\n",
        "- Experiment with batch normalization.\n",
        "- Add early stopping using a validation set.\n",
        "\n"
      ],
      "metadata": {
        "id": "0GuBrbaVJ9lf"
      }
    }
  ]
}